{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Runpod Instructions\n",
    "\n",
    "Steps:\n",
    "1) go to runpod\n",
    "2) create a new pod on A6000 NVIDIA GPU\n",
    "3) use template for pytorch=2.0.1\n",
    "4) ensure port 8888 is available to run jupyter notebook on \n",
    "goto: https://github.com/haotian-liu/LLaVA#demo\n",
    "run following commands\n",
    "```\n",
    "git clone https://github.com/haotian-liu/LLaVA.git\n",
    "cd LLaVA\n",
    "pip install --upgrade pip  # enable PEP 660 support\n",
    "pip install -e .\n",
    "```\n",
    "\n",
    "- Create an `/Images` folder to upload images to\n",
    "- After installing, can run the following code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llava.model.builder import load_pretrained_model\n",
    "from llava.mm_utils import get_model_name_from_path\n",
    "from llava.eval.run_llava import eval_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import base64\n",
    "import os\n",
    "\n",
    "folder = \"/workspace/Images\"\n",
    "def process_images(folder):\n",
    "    images_base64 = []\n",
    "    for filename in os.listdir(folder):\n",
    "        if filename.endswith(\".jpg\") or filename.endswith(\".png\"): \n",
    "            with open(os.path.join(folder, filename), \"rb\") as image_file:\n",
    "                encoded_string = base64.b64encode(image_file.read()).decode('utf-8')\n",
    "                images_base64.append(encoded_string)\n",
    "                print(f\"encoded: {filename}\")\n",
    "    return images_base64\n",
    "\n",
    "\n",
    "def query_images(images_list, n = None):\n",
    "    \"\"\"\n",
    "    Queries a list of images and generates captions for each image using a pre-trained model.\n",
    "\n",
    "    Args:\n",
    "        images_list (list): A list of image paths.\n",
    "        n (int, optional): The number of images to process. If not specified, all images in the list will be processed.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: A DataFrame containing the generated captions for each image.\n",
    "    \"\"\"\n",
    "\n",
    "    if n == None:\n",
    "        n = len(images_list)\n",
    "    model_path = \"liuhaotian/llava-v1.5-7b\"\n",
    "    caption_df = pd.DataFrame(columns=[\"caption\"])\n",
    "    tokenizer, model, image_processor, context_len = load_pretrained_model(\n",
    "        model_path=model_path,\n",
    "        model_base=None,\n",
    "        model_name=get_model_name_from_path(model_path)\n",
    "    )\n",
    "    \n",
    "    model_path = \"liuhaotian/llava-v1.5-7b\"\n",
    "    prompt = \"You are a captioning agent that takes one description of an image and reformats it into a new, clean concise description matching the following format:\\\n",
    "                a <describe the composition of image, closeup, medium etc> photo of a <describe woman> wearing <describe features of clothing, colour, texture> swimwear in <describe forground><describe background> <describe the mood>, \\\n",
    "                <other keywords used to describe the scene><other keywords of objects in image> return only the formatted text. only use the word swimwear to describe her clothing, nothing other than swimwear\"\n",
    "    for i, image in enumerate(images[:n]):\n",
    "        print(f\"processing: {i}\")\n",
    "        args = type('Args', (), {\n",
    "            \"model_path\": model_path,\n",
    "            \"model_base\": None,\n",
    "            \"model_name\": get_model_name_from_path(model_path),\n",
    "            \"query\": prompt,\n",
    "            \"conv_mode\": None,\n",
    "            \"image_file\": image,\n",
    "            \"sep\": \",\",\n",
    "            \"temperature\": 0,\n",
    "            \"top_p\": None,\n",
    "            \"num_beams\": 1,\n",
    "            \"max_new_tokens\": 512\n",
    "        })() \n",
    "        response = eval_model(args)\n",
    "        print(response)\n",
    "        caption_df = pd.concat([caption_df, pd.DataFrame([response], columns=[\"caption\"])], ignore_index=True)\n",
    "    return caption_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing multiple images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import os\n",
    "import gc  # Garbage Collector\n",
    "\n",
    "def replace_words(original_text, words_to_replace, replacement):\n",
    "    \"\"\"\n",
    "    Replaces any word in the original_text that matches a word in words_to_replace with replacement.\n",
    "\n",
    "    :param original_text: The original text as a string.\n",
    "    :param words_to_replace: A list of words to be replaced.\n",
    "    :param replacement: The string to replace the words with.\n",
    "    :return: The modified text with words replaced.\n",
    "    \"\"\"\n",
    "    for word in words_to_replace:\n",
    "        original_text = original_text.replace(word, replacement)\n",
    "    return original_text\n",
    "\n",
    "def process_images(folder):\n",
    "    images_paths = []\n",
    "    for filename in os.listdir(folder):\n",
    "        if filename.endswith(\".jpg\") or filename.endswith(\".png\"): \n",
    "            images_paths.append(os.path.join(folder, filename))\n",
    "    return images_paths\n",
    "\n",
    "def resize_image(image_path, output_folder, max_size=800):\n",
    "    try:\n",
    "        with Image.open(image_path) as img:\n",
    "            ratio = min(max_size / img.size[0], max_size / img.size[1])\n",
    "            new_size = (int(img.size[0] * ratio), int(img.size[1] * ratio))\n",
    "            resized_img = img.resize(new_size, Image.ANTIALIAS)\n",
    "\n",
    "            # Save the resized image to a new file\n",
    "            base_name = os.path.basename(image_path)\n",
    "            new_path = os.path.join(output_folder, base_name)\n",
    "            resized_img.save(new_path)\n",
    "            return new_path\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {image_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def query_images(images_paths, n=None):\n",
    "    if n is None:\n",
    "        n = len(images_paths)\n",
    "    output_folder = \"workspace/Images/resized\"\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    model_path = \"liuhaotian/llava-v1.5-7b\"\n",
    "    caption_df = pd.DataFrame(columns=[\"filename\",\"caption\"])\n",
    "    tokenizer, model, image_processor, context_len = load_pretrained_model(\n",
    "        model_path=model_path,\n",
    "        model_base=None,\n",
    "        model_name=get_model_name_from_path(model_path)\n",
    "    )\n",
    "\n",
    "    prompt1 = \"Verbosely describe the image with the following format a 'a (the composition, closeup, medium, portrait, ) photo of a (describe woman, hair colour, skin tone, body shape) wearing (describe features of swimwear, colour, pattern) swimwear in (describe scene in forground/background) (describe mood)'\"\n",
    "    prompt2 = \"write 5 key words corresponding to the following categories in this format: (scene), (colour), (mood), (lighting), (background)\"\n",
    "    \n",
    "    for i, image_path in enumerate(images_paths[:n]):\n",
    "        \n",
    "        resized_image_path = resize_image(image_path, output_folder)\n",
    "        filename = resized_image_path.split(\"/\")[-1]\n",
    "        print(f\"Processing: {filename}\")\n",
    "        args = type('Args', (), {\n",
    "            \"model_path\": model_path,\n",
    "            \"model_base\": None,\n",
    "            \"model_name\": get_model_name_from_path(model_path),\n",
    "            \"query\": prompt1,\n",
    "            \"conv_mode\": None,\n",
    "            \"image_file\": resized_image_path,\n",
    "            \"sep\": \",\",\n",
    "            \"temperature\": 0,\n",
    "            \"top_p\": None,\n",
    "            \"num_beams\": 1,\n",
    "            \"max_new_tokens\": 512\n",
    "        })() \n",
    "        response = replace_words(eval_model(args), [\"bikini\", \"swimsuit\", \"underwear\", \"lingerie\", \"panties\"], \"swimwear\")\n",
    "        args = type('Args', (), {\n",
    "            \"model_path\": model_path,\n",
    "            \"model_base\": None,\n",
    "            \"model_name\": get_model_name_from_path(model_path),\n",
    "            \"query\": prompt2,\n",
    "            \"conv_mode\": None,\n",
    "            \"image_file\": resized_image_path,\n",
    "            \"sep\": \",\",\n",
    "            \"temperature\": 0,\n",
    "            \"top_p\": None,\n",
    "            \"num_beams\": 1,\n",
    "            \"max_new_tokens\": 121\n",
    "        })() \n",
    "        response += \" \" + eval_model(args)\n",
    "        print(response)\n",
    "        caption_df = pd.concat([caption_df, pd.DataFrame([(filename, response)], columns=[\"filename\", \"caption\"])], ignore_index=True)\n",
    "\n",
    "        gc.collect()\n",
    "\n",
    "    return caption_df\n",
    "query_images(process_images(folder), 3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
